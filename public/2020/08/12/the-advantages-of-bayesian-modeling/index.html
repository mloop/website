<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.74.3" />


<title>The advantages of Bayesian modeling - Matthew Loop, PhD FAHA</title>
<meta property="og:title" content="The advantages of Bayesian modeling - Matthew Loop, PhD FAHA">


  <link href='/favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.jpg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/mloop">GitHub</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">8 min read</span>
    

    <h1 class="article-title">The advantages of Bayesian modeling</h1>

    
    <span class="article-date">2020-08-12</span>
    

    <div class="article-content">
      


<p><em>Recently I was asked by a colleague what the benefits of Bayesian analysis were. This essay represents my thinking as of today, about 10 years into being a statistician and ~2 years actually practicing some Bayesian analysis.</em></p>
<p><em>We all want to fit models to data that increase our understanding and make clear what we should do next.</em> As a population health scientist, I’m often interested in estimating the effect of an exposure on an outcome. Obviously that’s complicated by issues like unmeasured confounding, but we try to get as close as possible. Once an effect is established, other questions come up. How many people do we need to treat to prevent one case of the disease? Is the intervention cost effective? What would we predict differently about a person who is at risk for this disease now, that we didn’t know before? We hope our analyses of health data lead us to a clear next step, be it the next experiment or designing a new intervention.</p>
<p><em>The problem is that data are complex</em>. They do not fit the textbook examples we had in class. For example, real data can have missing values, variables can have known measurement error, and observations can be correlated (<em>e.g.</em>, because of neighborhood effects on health). But because these issues can require niche methods from the classical/Frequentist statistics literature, we often don’t implement them under the burden of grant and/or collaborator deadlines. So we note these issues as limitations in the discussion section, leaving ourselves feeling not quite satisfied Scientifically. We didn’t take into account as much as we could have, and who knows what effect that would have on our conclusions? Maybe we are leading ourselves astray (as well as others)? And as good Scientists we all know that we should be skeptical about our own ideas. Feynman said it well in his <a href="http://calteches.library.caltech.edu/51/2/CargoCult.htm">1974 commencement address to Caltech</a>:</p>
<blockquote>
<p>I’m talking about a specific, extra type of integrity that is not lying, but bending over backwards to show how you’re maybe wrong, that you ought to do when acting as a scientist. And this is our responsibility as scientists, certainly to other scientists, and I think to laymen.</p>
</blockquote>
<p>Unfortunately, the complexities of real world data often overwhelm the data analysis tools we have at our disposal.</p>
<p><em>That’s why I’ve come to love Bayesian data analysis</em>. You often don’t have to make assumptions you know to be false <em>and could possibly affect your results</em>. Many approaches built into modern Bayesian software directly deal real world complexities, such as measurement error or laboratory values missing because they are below the limit of detection for the assay. And I’ve always had a certain fascination with the ability to account for any uncertainty in your thinking and have that uncertainty fully propagate through to your final conclusions. It’s Scientific integrity, as Feynman defined it, in action.</p>
<p>Approaching a data analysis from the Bayesian perspective allows three main advantages: uncertainty is easy to interpret, uncertainty is easy to propagate through the analysis, and you can estimate basically any model you can write down.</p>
<div id="uncertainty-is-easy-to-interpret" class="section level2">
<h2>Uncertainty is easy to interpret<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></h2>
<p>When you took your first introductory statistics course, the instructor probably emphasized that the follow interpretation of a confidence interval is incorrect:</p>
<blockquote>
<p>The probability that the true value lies inside this interval is 0.95.</p>
</blockquote>
<p>Rather, the correct interpretation is more convoluted:</p>
<blockquote>
<p>If I were to repeat this experiment over and over and estimate this interval the same way every time, then the proportion of intervals that would contain the true value is 0.95.</p>
</blockquote>
<p>The difference in wording is fairly minor, and you can interpret the interval as the true values generally consistent with the data. But a big advantage of Bayesian analysis is that 95% <em>credible intervals</em> do have the first interpretation. That is, an interval for an odds ratio that is 1.1 - 1.2 means that you are 95% sure<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> that the true odds ratio is within that interval. And a 95% credible interval isn’t the only useful summary of Bayesian parameter estimates. You can ask other questions, such as, “What’s the probability that the odds ratio is greater than 1? Less than 1? What’s the probability that it is greater than 1.1, which I have determined is a clinically significant association?” Understanding the distribution of uncertainty about our estimates is very natural in a Bayesian analysis.</p>
</div>
<div id="uncertainty-is-easy-to-propagate-through-the-analysis" class="section level2">
<h2>Uncertainty is easy to propagate through the analysis</h2>
<p>This benefit is a bit vague because there are so many instances where it comes up. One example that I will give is multiple imputation. As of right now, <code>Stan</code> (the Bayesian software that I use) cannot impute categorical variables because of technical reasons. So when I have to use multiple imputation to take care of some missing data, I will sometimes use the <code>mice</code> package, just like I would if doing a Frequentist analysis and create, say, 10 imputed datasets. If I’m using the Frequentist approach I have to go back and remember Rubin’s Rules for how to combine the variances to get the right overall standard error for a parameter of interest (say, an odds ratio). But with a Bayesian approach, you fit the model to each of the 10 datasets and just concatenate all 10 posterior distributions for the odds ratio to obtain the appropriate posterior. And this posterior accounts for the uncertainty in the missing value imputations. No Rubin’s Rules to remember. This propagation of uncertainty lets you come up with credible intervals for all kinds of parameters which would be impossible to do analytically from the Frequentist perspective.</p>
</div>
<div id="you-can-estimate-basically-any-model-you-can-write-down" class="section level2">
<h2>You can estimate basically any model you can write down</h2>
<p>The final “nail in the coffin” on using Frequentist strategies to tackle complex data came for me while mentoring a GRA on a statistical analysis of youth living with HIV. Anyone familiar with HIV studies will know that viral load is an important confounder to take into account, but medications for HIV are so good these days that many participants in a study will have an undetectable viral load. If the particular assay being used has a limit of detection of 50 copies/mL, then all you know is that the participant’s viral load is between 0 and 50. While censored data can be analyzed using many different methods if it is the outcome (e.g., survival analysis), a censored covariate is a more difficult problem to attack. We eventually implemented the method described <a href="https://pubmed.ncbi.nlm.nih.gov/21710558/">here</a> in the new <code>R</code> package <a href="https://cran.rstudio.com/web/packages/lodr/index.html"><code>lodr</code></a>. Although the GRA worked very hard and diligently, it took about a year to learn the method, learn <code>C++</code>, implement the method, debug, and finally publish an <code>R</code> package. When I used a Bayesian approach to treat the censored values as parameters to be estimated, even though I was just a beginning Bayesian I had the method implemented in ~2 hours. Regardless of actual computation time (which was about the same), the difference in <em>development time</em> to get a working solution was incredible.</p>
<p>The solution hinged on the fact that everything that’s known in a Bayesian analysis is data and everything that’s unknown is a parameter. A basic linear regression model might look something like this:</p>
<p><span class="math display">\[y \sim N(\mu, \sigma^2)\]</span></p>
<p><span class="math display">\[\mu = \beta_0 + x_1\beta_1\]</span></p>
<p>For the participants without a measured value of <span class="math inline">\(x_1\)</span>, all you have to do is assume they come from the same distribution as the observed values but need to be estimated. In other words, for those with values below the limit of detection, you can assume</p>
<p><span class="math display">\[x_1 \sim truncN(\mu_{x_1}, \sigma^2_{x_1}, 0, 50),\]</span> where 0 represents the lower bound of the missing values and 50 represents the upper bound of the missing values. Bayesian software will sample (or “impute”) those missing values on the fly and fit the overall regression model you are interested in, accounting for the uncertainty in the imputed values. Basically, as long as you can write down a model you believe to be reasonable for the missing values (the missing values could have depended on other variables without any real added complication to the estimation), you can fit that model in Bayesian data analysis.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>My first strategy with a data analysis is to do a Bayesian analysis. It is hard to justify using the Frequentist approach, besides lack of familiarity with it on the part of collaborators. But increased using of Bayesian strategies I think will lead to better Science and better understanding of the question Scientists always ask themselves after finishing a paper: “What should I do next?”</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>One of my PhD committee members, George Howard, rightly asks where the evidence is that Bayesian analysis is easier to interpret. I am not aware of any studies that directly provide evidence. But there are plenty of studies that show that anyone from physicians to PhD statisticians make mistakes in interpreting p-values and/or confidence intervals.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>The word “sure” also emphasizes that Bayesian analysis and Frequentist analysis view probability differently. Bayesian analysis views probability as a belief or a bet, while Frequentist analysis views probability as a proportion over a hypothetically long run of extra experiments. Given we usually only have one experiment in population health, I find the Bayesian perspective more practical.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Just like maximum likelihood, Monte Carlo methods that are used to fit Bayesian models have their own idiosyncracies and failures. Sometimes the model doesn’t converge and you have to figure out what to do next. But I’ve found those issues much less common than issues fitting, say, mixed effects models using maximum likelihood.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

